{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCnh7txruDwL"
   },
   "source": [
    "# Поиск аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6II8JMnuDwO"
   },
   "source": [
    "Методы обнаружения аномалий, как следует из названия, позволяют находить необычные объекты в выборке. Но что такое \"необычные\" и совпадает ли это определение у разных методов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFjf40SeuDwO"
   },
   "source": [
    "Начнём с поиска аномалий в текстах: научимся отличать вопросы о программировании от текстов из 20newsgroups про религию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIwHXueZuDwO"
   },
   "source": [
    "Подготовьте данные: в обучающую выборку возьмите 20 тысяч текстов из датасета Stack Overflow, а тестовую выборку сформируйте из 10 тысяч текстов со Stack Overflow и 100 текстов из класса soc.religion.christian датасета 20newsgroups (очень пригодится функция `fetch_20newsgroups(categories=['soc.religion.christian'])`). Тексты про программирование будем считать обычными, а тексты про религию — аномальными."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KJgW4zVZuDwP",
    "ExecuteTime": {
     "end_time": "2026-02-13T19:37:11.114509Z",
     "start_time": "2026-02-13T19:37:05.883224Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "so = pd.read_parquet(\"stackoverflow-posts-00000-of-00058.parquet\")\n",
    "\n",
    "so = pd.read_parquet(\n",
    "    \"stackoverflow-posts-00000-of-00058.parquet\",\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "\n",
    "texts = (so[\"Title\"].fillna(\"\") + \" \" + so[\"Body\"].fillna(\"\"))[:30000].values\n",
    "\n",
    "\n",
    "train_so = texts[:20000]\n",
    "test_so = texts[20000:30000]\n",
    "\n",
    "religion_texts = []\n",
    "\n",
    "religion_texts = []\n",
    "folder = \"20news-bydate-train/soc.religion.christian\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    with open(os.path.join(folder, filename), \"r\", encoding=\"latin1\") as f:\n",
    "        religion_texts.append(f.read())\n",
    "\n",
    "print(\"Всего религиозных текстов:\", len(religion_texts))\n",
    "\n",
    "test_religion = religion_texts[:100]\n",
    "\n",
    "X_train_texts = train_so\n",
    "X_test_texts = list(test_so) + test_religion\n",
    "y_test = np.array([0]*len(test_so) + [1]*len(test_religion))\n",
    "\n",
    "print(\"Train:\", len(X_train_texts))\n",
    "print(\"Test:\", len(X_test_texts))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего религиозных текстов: 599\n",
      "Train: 20000\n",
      "Test: 10100\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0cVmrBDuDwP"
   },
   "source": [
    "**(1 балл)**\n",
    "\n",
    "Проверьте качество выделения аномалий (precision и recall на тестовой выборке, если считать аномалии положительным классов, а обычные тексты — отрицательным) для IsolationForest. В качестве признаков используйте TF-IDF, где словарь и IDF строятся по обучающей выборке. Не забудьте подобрать гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9C0ZzIGFuDwQ",
    "ExecuteTime": {
     "end_time": "2026-02-13T19:45:59.687403Z",
     "start_time": "2026-02-13T19:45:56.953565Z"
    }
   },
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english'\n",
    ")\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "clf = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    max_samples='auto',\n",
    "    contamination=len(test_religion)/X_test.shape[0],\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_pred_binary = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.044\n",
      "Recall: 0.050\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EEXad6euDwQ"
   },
   "source": [
    "**(5 баллов)**\n",
    "\n",
    "Скорее всего, качество оказалось не на высоте. Разберитесь, в чём дело:\n",
    "* посмотрите на тексты, которые выделяются как аномальные, а также на слова, соответствующие их ненулевым признакам\n",
    "* изучите признаки аномальных текстов\n",
    "* посмотрите на тексты из обучающей выборки, ближайшие к аномальным; действительно ли они похожи по признакам?\n",
    "\n",
    "Сделайте выводы и придумайте, как избавиться от этих проблем. Предложите варианты двух типов: (1) в рамках этих же признаков (но которые, возможно, будут считаться по другим наборам данных) и методов и (2) без ограничений на изменения. Реализуйте эти варианты и проверьте их качество."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xYvrEzFGuDwR",
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:03.682921Z",
     "start_time": "2026-02-13T19:46:03.667957Z"
    }
   },
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "anomaly_indices = np.where(y_pred_binary == 1)[0]\n",
    "\n",
    "print(\"Первые 5 аномальных текстов с ключевыми словами:\")\n",
    "for idx in anomaly_indices[:5]:\n",
    "    text = X_test_texts[idx]\n",
    "    tfidf_row = X_test[idx].toarray().flatten()  # длина = feature_names.shape[0]\n",
    "    nonzero_indices = np.where(tfidf_row > 0)[0]  # индексы только в диапазоне 0..max_features-1\n",
    "    top_words = feature_names[nonzero_indices][:10]  # безопасно\n",
    "    print(f\"\\nТекст {idx}: {text[:200]}...\")\n",
    "    print(\"Слова признаков:\", top_words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 аномальных текстов с ключевыми словами:\n",
      "\n",
      "Текст 28:  \n",
      "1. The question is not really related to checked vs. unchecked debate, the same applies to both exception types.\n",
      "2. Between the point where the ConstraintViolationException is thrown and the point, ...\n",
      "Слова признаков: ['abort' 'action' 'additional' 'applies' 'available' 'calls' 'cancel'\n",
      " 'care' 'catch' 'checked']\n",
      "\n",
      "Текст 271:  The book 'Real-Time Collision Detection' by Christer Ericson (ISBN: 1-55860-732-3) is a recent (2005) and widely praised book which should give you some good answers.\n",
      "\n",
      "It starts with a basic primer o...\n",
      "Слова признаков: ['2005' 'actually' 'algorithms' 'aligned' 'allows' 'answers' 'avoid'\n",
      " 'aware' 'away' 'axis']\n",
      "\n",
      "Текст 313:  A tool is a big help.\n",
      "\n",
      "However, there are times when you can't use a tool: the heap dump is so huge it crashes the tool, you are trying to troubleshoot a machine in some production environment to whi...\n",
      "Слова признаков: ['access' 'act' 'allocated' 'allocation' 'aren' 'arr' 'backward' 'bad'\n",
      " 'begin' 'big']\n",
      "\n",
      "Текст 324: What is the best way to migrate an existing messy webapp to elegant MVC? I joined a new company about a month ago. The company is rather small in size and has pretty strong \"start-up\" feel to it. I'm ...\n",
      "Слова признаков: ['access' 'add' 'ago' 'app' 'application' 'architect' 'architecture'\n",
      " 'areas' 'beans' 'beginning']\n",
      "\n",
      "Текст 355:  I think the \"pure OOP\" answer is that if operations on the class are invalid when certain members aren't initialized, then these members must be set by the constructor.  There's always the case where...\n",
      "Слова признаков: ['answer' 'api' 'approach' 'aren' 'assume' 'based' 'behaves' 'best'\n",
      " 'burden' 'calling']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHk8G7mLuDwR"
   },
   "source": [
    "### Эксперимент только с изменением датасета"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bj0a2TOCuDwR",
    "ExecuteTime": {
     "end_time": "2026-02-13T19:48:12.149312Z",
     "start_time": "2026-02-13T19:48:09.575319Z"
    }
   },
   "source": [
    "#code here\n",
    "import numpy as np\n",
    "\n",
    "# Вычисляем TF-IDF для train\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_texts)\n",
    "\n",
    "# Находим \"редкие\" строки: среднее TF-IDF > threshold\n",
    "row_means = X_train_tfidf.mean(axis=1).A.flatten()\n",
    "threshold = np.percentile(row_means, 5)  # нижние 5% редких\n",
    "filtered_indices = np.where(row_means > threshold)[0]\n",
    "\n",
    "X_train_filtered = X_train_tfidf[filtered_indices]\n",
    "clf = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    max_samples='auto',\n",
    "    contamination=len(test_religion)/X_test.shape[0],\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train_filtered)\n",
    "X_test_tfidf = vectorizer.transform(X_test_texts)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "y_pred_binary = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.098, Recall: 0.090\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_53_1ftuDwR"
   },
   "source": [
    "### Эксперимент с любыми изменениями"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u515SJs2uDwS",
    "ExecuteTime": {
     "end_time": "2026-02-13T20:23:00.445834Z",
     "start_time": "2026-02-13T20:22:45.601362Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=25000,\n",
    "    ngram_range=(1,3),\n",
    "    stop_words=None\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "clf = IsolationForest(\n",
    "    n_estimators=500,\n",
    "    max_samples='auto',\n",
    "    contamination=len(test_religion)/X_test.shape[0],\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_binary = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "# Метрики\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.235\n",
      "Recall: 0.230\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukGE7udQuDwS"
   },
   "source": [
    "Подготовьте выборку: удалите столбцы `['id', 'date', 'price', 'zipcode']`, сформируйте обучающую и тестовую выборки по 10 тысяч домов.\n",
    "\n",
    "Добавьте в тестовую выборку 10 новых объектов, в каждом из которых испорчен ровно один признак — например, это может быть дом из другого полушария, из далёкого прошлого или будущего, с площадью в целый штат или с таким числом этажей, что самолётам неплохо бы его облетать стороной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-0yaOlyuDwS"
   },
   "source": [
    "Посмотрим на методы обнаружения аномалий на более простых данных — уж на табличном датасете с 19 признаками всё должно работать как надо!\n",
    "\n",
    "Скачайте данные о стоимости домов: https://www.kaggle.com/harlfoxem/housesalesprediction/data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lg-T_gYEuDwS"
   },
   "source": [
    "#code here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkkxswUOuDwS"
   },
   "source": [
    "**Задание 9. (2 балла)**\n",
    "\n",
    "Примените IsolationForest для поиска аномалий в этих данных, запишите их качество (как и раньше, это precision и recall). Проведите исследование:\n",
    "\n",
    "Нарисуйте распределения всех признаков и обозначьте на этих распределениях объекты, которые признаны аномальными."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-q8DQDG7uDwS"
   },
   "source": [
    "#code here"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
