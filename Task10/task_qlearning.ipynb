{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dvm6MmIGddQw",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.356280Z",
     "start_time": "2026-02-07T20:24:25.336903Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgcJ2LJ2ddQz"
   },
   "source": [
    "## 1. Q-learning in the wild (3 pts)\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c6uJVRBJddQ1",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.381304Z",
     "start_time": "2026-02-07T20:24:25.363355Z"
    }
   },
   "source": [
    "import random,math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent():\n",
    "  \"\"\"\n",
    "    Q-Learning Agent\n",
    "\n",
    "    Instance variables you have access to\n",
    "      - self.epsilon (exploration prob)\n",
    "      - self.alpha (learning rate)\n",
    "      - self.discount (discount rate aka gamma)\n",
    "\n",
    "    Functions you should use\n",
    "      - self.getLegalActions(state)\n",
    "        which returns legal actions for a state\n",
    "      - self.getQValue(state,action)\n",
    "        which returns Q(state,action)\n",
    "      - self.setQValue(state,action,value)\n",
    "        which sets Q(state,action) := value\n",
    "\n",
    "    !!!Important!!!\n",
    "    NOTE: please avoid using self._qValues directly to make code cleaner\n",
    "  \"\"\"\n",
    "  def __init__(self,alpha,epsilon,discount,getLegalActions):\n",
    "    \"We initialize agent and Q-values here.\"\n",
    "    self.getLegalActions= getLegalActions\n",
    "    self._qValues = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    self.alpha = alpha\n",
    "    self.epsilon = epsilon\n",
    "    self.discount = discount\n",
    "\n",
    "  def getQValue(self, state, action):\n",
    "    #print(state)\n",
    "    #print(action)\n",
    "    if not (state in self._qValues) or not (action in self._qValues[state]):\n",
    "        return 0.0\n",
    "    return self._qValues[state][action]\n",
    "\n",
    "  def setQValue(self,state,action,value):\n",
    "    \"\"\"\n",
    "      Sets the Qvalue for [state,action] to the given value\n",
    "    \"\"\"\n",
    "    self._qValues[state][action] = value\n",
    "\n",
    "#---------------------#start of your code#---------------------#\n",
    "\n",
    "  def getValue(self, state):\n",
    "    \"\"\"\n",
    "      Returns max_action Q(state,action)\n",
    "      where the max is over legal actions.\n",
    "    \"\"\"\n",
    "\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "    #If there are no legal actions, return 0.0\n",
    "    if len(possibleActions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "    max_q_value = max([self.getQValue(state, action) for action in possibleActions])\n",
    "    return max_q_value\n",
    "\n",
    "  def getPolicy(self, state):\n",
    "    \"\"\"\n",
    "      Compute the best action to take in a state.\n",
    "\n",
    "    \"\"\"\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "            return None\n",
    "    best_q_value = self.getValue(state)\n",
    "    best_actions = [action for action in possibleActions if abs(self.getQValue(state, action) - best_q_value) < 1e-10]\n",
    "\n",
    "    return random.choice(best_actions)\n",
    "\n",
    "  def getAction(self, state):\n",
    "    \"\"\"\n",
    "      Compute the action to take in the current state, including exploration.\n",
    "\n",
    "      With probability self.epsilon, we should take a random action.\n",
    "      otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "      HINT: You might want to use util.flipCoin(prob)\n",
    "      HINT: To pick randomly from a list, use random.choice(list)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick Action\n",
    "    possibleActions = self.getLegalActions(state)\n",
    "    #If there are no legal actions, return None\n",
    "    if len(possibleActions) == 0:\n",
    "            return None\n",
    "    #choose action with epsilon exploration strategy:\n",
    "    if random.random() < self.epsilon:\n",
    "            action = random.choice(possibleActions)\n",
    "    else:\n",
    "            action = self.getPolicy(state)\n",
    "\n",
    "    return action\n",
    "\n",
    "  def update(self, state, action, nextState, reward):\n",
    "    \"\"\"\n",
    "      You should do your Q-Value update here\n",
    "\n",
    "      NOTE: You should never call this function,\n",
    "      it will be called on your behalf\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #agent parameters\n",
    "    gamma = self.discount\n",
    "    learning_rate = self.alpha\n",
    "\n",
    "    current_q = self.getQValue(state, action)\n",
    "    next_max_q = self.getValue(nextState)\n",
    "\n",
    "    new_q = current_q + learning_rate * (reward + gamma * next_max_q - current_q)\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    self.setQValue(state, action, new_q)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6IyXE8SzddQ2",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.403088Z",
     "start_time": "2026-02-07T20:24:25.387806Z"
    }
   },
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7TFzQrJJddQ2",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.422656Z",
     "start_time": "2026-02-07T20:24:25.408972Z"
    }
   },
   "source": [
    "def play_and_train(env,agent,t_max=10**4):\n",
    "    \"\"\"This function should\n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.getAction(s)\n",
    "\n",
    "        next_s,r,done,_ = env.step(a)\n",
    "\n",
    "        #<train(update) agent for state s>\n",
    "        agent.update(s, a, next_s, r)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "\n",
    "    return total_reward"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LhsvXf9BddQ2",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.442511Z",
     "start_time": "2026-02-07T20:24:25.428855Z"
    }
   },
   "source": [
    "agent = QLearningAgent(alpha=0.15,epsilon=0.15,discount=0.96,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZwJKCfvddQ3"
   },
   "source": [
    "Достигните положительной награды, постройте график"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zo6QbdGxddQ3",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.461034Z",
     "start_time": "2026-02-07T20:24:25.447912Z"
    }
   },
   "source": [
    "from IPython.display import clear_output\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "48CrrrxGddQ3",
    "ExecuteTime": {
     "end_time": "2026-02-07T20:24:25.520876Z",
     "start_time": "2026-02-07T20:24:25.470971Z"
    }
   },
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "initial_epsilon = 0.53\n",
    "final_epsilon = 0.011\n",
    "decay_rate = 0.996\n",
    "\n",
    "rewards = []\n",
    "epsilon_history = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))\n",
    "    agent.epsilon = max(final_epsilon, initial_epsilon * (decay_rate ** i))\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    if i % 100 ==99:\n",
    "        clear_output(True)\n",
    "        print(agent.epsilon)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "print(rewards[-1])"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m epsilon_history \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000\u001B[39m):\n\u001B[1;32m---> 10\u001B[0m     rewards\u001B[38;5;241m.\u001B[39mappend(\u001B[43mplay_and_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43magent\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     11\u001B[0m     agent\u001B[38;5;241m.\u001B[39mepsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(final_epsilon, initial_epsilon \u001B[38;5;241m*\u001B[39m (decay_rate \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m i))\n\u001B[0;32m     12\u001B[0m     epsilon_history\u001B[38;5;241m.\u001B[39mappend(agent\u001B[38;5;241m.\u001B[39mepsilon)\n",
      "Cell \u001B[1;32mIn[16], line 12\u001B[0m, in \u001B[0;36mplay_and_train\u001B[1;34m(env, agent, t_max)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(t_max):\n\u001B[0;32m     10\u001B[0m     a \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mgetAction(s)\n\u001B[1;32m---> 12\u001B[0m     next_s,r,done,_ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m#<train(update) agent for state s>\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     agent\u001B[38;5;241m.\u001B[39mupdate(s, a, next_s, r)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_step_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[1;34m(env, action)\u001B[0m\n\u001B[0;32m    230\u001B[0m obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m result\n\u001B[0;32m    232\u001B[0m \u001B[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001B[39;00m\n\u001B[1;32m--> 233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(terminated, (\u001B[38;5;28mbool\u001B[39m, \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbool8\u001B[49m)):\n\u001B[0;32m    234\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(terminated)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    236\u001B[0m     )\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncated, (\u001B[38;5;28mbool\u001B[39m, np\u001B[38;5;241m.\u001B[39mbool8)):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py:427\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(attr)\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchar\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mchar\u001B[39;00m\n\u001B[0;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m char\u001B[38;5;241m.\u001B[39mchararray\n\u001B[1;32m--> 427\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    428\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;18m__name__\u001B[39m, attr))\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "gxQu70I_ddQ4"
   },
   "source": [
    "## 3. Continuous state space (2 pt)\n",
    "\n",
    "Чтобы использовать табличный q-learning на continuous состояниях, надо как-то их обрабатывать и бинаризовать. Придумайте способ разбивки на дискретные состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4eD9gTjddQ4"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_actions = env.action_space.n\n",
    "print(\"first state:%s\"%(env.reset()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6V68sMQddQ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUM1t0jvddQ4"
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "Постройте распределения различных частей состояния игры. Сыграйте несколько игр и запишите все состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnHeBx5XddQ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zZHwCUfddQ5"
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6f-DyizddQ5"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "class Binarizer(ObservationWrapper):\n",
    "\n",
    "    def to_bin(self, value, bins):\n",
    "\n",
    "        return\n",
    "\n",
    "    def _observation(self,state):\n",
    "\n",
    "        state = (self.to_bin(state[0], ), self.to_bin(state[1], ), self.to_bin(state[2], ), self.to_bin(state[3], ))\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpaDIw43ddQ5"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkZCEBD_ddQ5"
   },
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sFMtxZYddQ5"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=,epsilon=,discount=,\n",
    "                       getLegalActions = lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2uuuA5bddQ5"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewBuf = []\n",
    "ma = -1000000000000\n",
    "for i in range(10000):\n",
    "    for i in range(100):\n",
    "        rewards.append(play_and_train(env,agent))\n",
    "    agent.epsilon *= #\n",
    "    rewBuf.append(np.mean(rewards[-100:]))\n",
    "    clear_output(True)\n",
    "    print(agent.epsilon)\n",
    "    print(rewBuf[-1])\n",
    "    plt.plot(rewBuf)\n",
    "    if(rewBuf[-1] > 195):\n",
    "        print(\"Win!\")\n",
    "        break\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GCEU6g1ddQ5"
   },
   "source": [
    "## 4. Experience replay (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ_3uojDddQ5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._replaceId = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize.\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        if len(self._storage) == self._maxsize:\n",
    "            #\n",
    "        else:\n",
    "            #\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "\n",
    "        return states, actions, rewards, next_states, is_done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KkoMo-EddQ6"
   },
   "source": [
    "Some tests to make sure your buffer works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze99MZrmddQ6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = tuple(range(5))\n",
    "obj2 = tuple(range(5, 10))\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1)==obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert tuple(np.unique(a) for a in replay.sample(100))==obj2\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100))==2\n",
    "replay.add(*obj1)\n",
    "assert tuple(np.unique(a) for a in replay.sample(100))==obj1\n",
    "print (\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaImoEEgddQ6"
   },
   "source": [
    "Now let's use this buffer to improve training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czwcYYYmddQ6"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = Binarizer(gym.make('CartPole-v0'))\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChtwaobcddQ6"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=,epsilon=,discount=,\n",
    "                       getLegalActions = lambda s: range(n_actions))\n",
    "replay = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdKXIULcddQ6"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4, batch_size=10):\n",
    "    \"\"\"This function should\n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        aсtion = agent.getAction(s)\n",
    "        next_s, r, done,_ = env.step(aсtion)\n",
    "\n",
    "        #заполните реплей\n",
    "        #опционально - моежте также как в варианте без реплея обучаться по состояниям которые\n",
    "\n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:break\n",
    "\n",
    "    #learn from replay\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "rcB2f2GaddQ6"
   },
   "source": [
    "Train with experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lqmk0S_ddQ6"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewBuf = []\n",
    "ma = -1000000000000\n",
    "for i in range(10000):\n",
    "    for i in range(100):\n",
    "        rewards.append(play_and_train(env,agent, batch_size=1000))\n",
    "    agent.epsilon *= #\n",
    "    rewBuf.append(np.mean(rewards[-100:]))\n",
    "    clear_output(True)\n",
    "    print(agent.epsilon)\n",
    "    print(rewBuf[-1])\n",
    "    plt.plot(rewBuf)\n",
    "    if(rewBuf[-1] > 195):\n",
    "        print(\"Win!\")\n",
    "        break\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
